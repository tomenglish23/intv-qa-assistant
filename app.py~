"""
Interview Q&A RAG Assistant - Flask API
"""
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
from typing import TypedDict, List

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

app = Flask(__name__)
CORS(app)

# Global variables
vector_store = None
doc_ingester = None
app_graph = None

DIR_DATA = "./data"
DIR_PERSIST = "./chroma_db"

# ============================================
# DOCUMENT INGESTION
# ============================================

class DocIngester:
    """Loads and parses markdown files with hierarchy awareness"""
    
    def __init__(self, data_dir: str = DIR_DATA):
        self.data_dir = data_dir
        self.level1 = set()
        self.level2 = {}
        self.level3 = {}
    
    def load_docs(self) -> List[Document]:
        """Load all .txt/.md files from data directory"""
        docs = []
        
        for filename in os.listdir(self.data_dir):
            if filename.endswith(('.txt', '.md')):
                filepath = os.path.join(self.data_dir, filename)
                print(f"?? Loading: {filename}")
                
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                docs.extend(self._parse_hierarchical(content, filename))
        
        print(f"? Chunks: Loaded {len(docs)} chunks total")
        print(f"?? Level 1: {len(self.level1)} disciplines")
        print(f"?? Level 2: {sum(len(areas) for areas in self.level2.values())} areas")
        return docs
    
    def get_level1(self) -> List[str]:
        """Get list of all Level 1 disciplines"""
        return sorted(self.level1)
    
    def get_level2(self) -> dict:
        """Get Level 2 areas organized by Level 1"""
        return {k: sorted(v) for k, v in self.level2.items()}
    
    def _parse_hierarchical(self, content: str, source: str) -> List[Document]:
        """Parse content tracking discipline/area context"""
        lines = content.split('\n')
        
        current_discipline = "General"
        current_area = "General"
        current_chunk_lines = []
        docs = []
        
        for line in lines:
            line = line.rstrip('\r')
            
            if line.startswith("# ") and not line.startswith("## "):
                if current_chunk_lines:
                    self._create_chunk(current_chunk_lines, current_discipline, current_area, source, docs)
                    current_chunk_lines = []
                
                current_discipline = line[2:].strip()
                self.level1.add(current_discipline)
                
                if current_discipline not in self.level2:
                    self.level2[current_discipline] = set()
                if current_discipline not in self.level3:
                    self.level3[current_discipline] = {}
                
                continue
            
            elif line.startswith("## ") and not line.startswith("### "):
                if current_chunk_lines:
                    self._create_chunk(current_chunk_lines, current_discipline, current_area, source, docs)
                    current_chunk_lines = []
                
                current_area = line[3:].strip()
                self.level2[current_discipline].add(current_area)
                
                if current_area not in self.level3[current_discipline]:
                    self.level3[current_discipline][current_area] = set()
                
                continue
            
            elif line.startswith("### ") or line.startswith("Q:"):
                if current_chunk_lines:
                    self._create_chunk(current_chunk_lines, current_discipline, current_area, source, docs)
                    current_chunk_lines = []
                
                topic = line[4:].strip() if line.startswith("### ") else line[2:].strip()
                if current_area in self.level3.get(current_discipline, {}):
                    self.level3[current_discipline][current_area].add(topic)
                
                current_chunk_lines.append(line)
            
            else:
                current_chunk_lines.append(line)
            
            chunk_text = '\n'.join(current_chunk_lines)
            if len(chunk_text) >= 2000:
                self._create_chunk(current_chunk_lines, current_discipline, current_area, source, docs)
                current_chunk_lines = []
        
        if current_chunk_lines:
            self._create_chunk(current_chunk_lines, current_discipline, current_area, source, docs)
        
        return docs
    
    def _create_chunk(self, lines, discipline, area, source, docs):
        """Create a document chunk with metadata"""
        chunk_text = '\n'.join(lines).strip()
        
        if not chunk_text:
            return
        
        docs.append(Document(
            page_content=chunk_text,
            metadata={
                "discipline": discipline,
                "area": area,
                "source": source
            }
        ))


# ============================================
# VECTOR STORE
# ============================================

class VectorStore:
    """Manages ChromaDB vector store"""
    
    def __init__(self, persist_dir: str = DIR_PERSIST):
        self.embs = OpenAIEmbeddings(model="text-embedding-3-small")
        self.persist_dir = persist_dir
        self.vs = None
    
    def create_or_load(self, docs: List[Document] = None):
        """Create new vectorstore or load existing"""
        if docs:
            self.vs = Chroma.from_documents(
                documents=docs,
                embedding=self.embs,
                persist_directory=self.persist_dir
            )
            print(f"? Vectorstore: Created with {len(docs)} docs")
        else:
            self.vs = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embs
            )
            print("? Vectorstore: Loaded existing")
        
        return self.vs
    
    def search(self, query: str, k: int = 4, filter_dict: dict = None):
        """Search with optional metadata filtering"""
        if filter_dict:
            return self.vs.similarity_search(query, k=k, filter=filter_dict)
        return self.vs.similarity_search(query, k=k)


# ============================================
# LANGGRAPH WORKFLOW
# ============================================

class RAGState(TypedDict):
    """State object for LangGraph workflow"""
    q: str
    discipline: str | None
    rxd_docs: List[Document]
    ctx: str
    a: str
    confidence: float
    use_fb: bool
    sources: List[str]


def create_rag_graph(vs: VectorStore):
    """Creates the LangGraph workflow"""
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    def analyze_query(state: RAGState) -> RAGState:
        """Extract discipline from question"""
        if state.get("discipline"):
            return state
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Extract the technical discipline from this question. "
                      "Options: C#, Angular, SQL, DevOps, General. Return ONLY the discipline name."),
            ("user", "{q}")
        ])
        
        chain = prompt | llm
        response = chain.invoke({"q": state["q"]})
        discipline = response.content.strip()
        
        if "c sharp" in discipline.lower() or "csharp" in discipline.lower():
            discipline = "C#"
        
        state["discipline"] = discipline if discipline != "General" else None
        return state
    
    def retrieve_docs(state: RAGState) -> RAGState:
        """Retrieve relevant docs"""
        filter_dict = {"discipline": state["discipline"]} if state.get("discipline") else None
        docs = vs.search(state["q"], k=4, filter_dict=filter_dict)
        
        state["rxd_docs"] = docs
        state["ctx"] = "\n\n".join([doc.page_content for doc in docs])
        state["sources"] = [doc.metadata.get("source", "unknown") for doc in docs]
        return state
    
    def grade_relevance(state: RAGState) -> RAGState:
        """Determine if retrieved docs are relevant"""
        if not state["rxd_docs"]:
            state["use_fb"] = True
            state["confidence"] = 0.0
            return state
        
        q_words = set(state["q"].lower().split())
        ctx_words = set(state["ctx"].lower().split())
        overlap = len(q_words & ctx_words)
        confidence = min(overlap / max(len(q_words), 1), 1.0)
        
        state["confidence"] = confidence
        state["use_fb"] = confidence < 0.3
        return state
    
    def gen_a(state: RAGState) -> RAGState:
        """Generate answer from context"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert technical interviewer. "
                      "Answer based ONLY on the provided context. Be concise and accurate."),
            ("user", "Context:\n{ctx}\n\nQuestion: {q}\n\nAnswer:")
        ])
        
        chain = prompt | llm
        response = chain.invoke({"ctx": state["ctx"], "q": state["q"]})
        state["a"] = response.content
        return state
    
    def fallback_a(state: RAGState) -> RAGState:
        """Use OpenAI when docs don't have answer"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert technical interviewer. Answer this question accurately."),
            ("user", "{q}")
        ])
        
        chain = prompt | llm
        response = chain.invoke({"q": state["q"]})
        state["a"] = f"[Using general knowledge] {response.content}"
        return state
    
    def should_use_fb(state: RAGState) -> str:
        return "fallback" if state["use_fb"] else "generate"
    
    # Build graph
    workflow = StateGraph(RAGState)
    workflow.add_node("analyze", analyze_query)
    workflow.add_node("retrieve", retrieve_docs)
    workflow.add_node("grade", grade_relevance)
    workflow.add_node("generate", gen_a)
    workflow.add_node("fallback", fallback_a)
    
    workflow.set_entry_point("analyze")
    workflow.add_edge("analyze", "retrieve")
    workflow.add_edge("retrieve", "grade")
    workflow.add_conditional_edges("grade", should_use_fb, {"generate": "generate", "fallback": "fallback"})
    workflow.add_edge("generate", END)
    workflow.add_edge("fallback", END)
    
    return workflow.compile()


# ============================================
# INITIALIZATION
# ============================================

def initialize_system():
    """Initialize the RAG system on startup"""
    global vector_store, doc_ingester, app_graph
    
    print("=" * 70)
    print("Interview Q&A RAG Assistant - Initializing...")
    print("=" * 70)
    
    doc_ingester = DocIngester(data_dir=DIR_DATA)
    docs = doc_ingester.load_docs()
    
    if not docs:
        print("ERROR: No documents found!")
        return False
    
    vector_store = VectorStore()
    vector_store.create_or_load(docs)
    
    app_graph = create_rag_graph(vector_store)
    
    print("? System ready!")
    return True

initialize_system()


# ============================================
# API ROUTES
# ============================================

@app.route('/')
def index():
    """Health check"""
    return jsonify({
        "status": "ok",
        "service": "Interview Q&A RAG Assistant"
    })


@app.route('/api/query', methods=['POST'])
def query():
    """Handle query requests"""
    try:
        data = request.json
        question = data.get('question', '').strip()
        discipline = data.get('discipline')
        
        if not question:
            return jsonify({"error": "Question required"}), 400
        
        if not app_graph:
            return jsonify({"error": "System not initialized"}), 503
        
        result = app_graph.invoke({
            "q": question,
            "discipline": discipline,
            "rxd_docs": [],
            "ctx": "",
            "a": "",
            "confidence": 0.0,
            "use_fb": False,
            "sources": []
        })
        
        return jsonify({
            "answer": result["a"],
            "confidence": result["confidence"],
            "sources": result["sources"],
            "use_fallback": result["use_fb"]
        })
        
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/disciplines', methods=['GET'])
def get_disciplines():
    """Get all disciplines"""
    try:
        if not doc_ingester:
            return jsonify({"error": "Not initialized"}), 503
        
        return jsonify({"disciplines": doc_ingester.get_level1()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/stats', methods=['GET'])
def stats():
    """Get statistics"""
    try:
        if not doc_ingester:
            return jsonify({"error": "Not initialized"}), 503
        
        return jsonify({
            "disciplines": len(doc_ingester.level1),
            "areas": sum(len(areas) for areas in doc_ingester.level2.values()),
            "status": "ready"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)


